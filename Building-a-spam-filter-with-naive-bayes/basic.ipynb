{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e979928",
   "metadata": {},
   "source": [
    "# Building a Spam Filter with Naive Bayes\n",
    "\n",
    "We're going to study the practical side of the algorithm by building a spam filter for SMS messages.\n",
    "\n",
    "To classify messages as spam or non-spam, we saw in the previous lesson that the computer:\n",
    "\n",
    "- Learns how humans classify messages.\n",
    "- Uses that human knowledge to estimate probabilities for new messages — probabilities for spam and non-spam.\n",
    "- Classifies a new message based on these probability values — if the probability for spam is greater, then it classifies the message as spam. Otherwise, it classifies it as non-spam (if the two probability values are equal, then we may need a human to classify the message).\n",
    "\n",
    "So our first task is to \"teach\" the computer how to classify messages. To do that, we'll use the multinomial Naive Bayes algorithm along with a dataset of 5,572 SMS messages that are already classified by humans. You can also download the dataset directly from this [link](https://dq-content.s3.amazonaws.com/433/SMSSpamCollection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c384db5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sms = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, names=['Label', 'SMS'])\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53f922cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebc4d4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     86.593683\n",
       "spam    13.406317\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms['Label'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4ad464",
   "metadata": {},
   "source": [
    "## Training and Test Set\n",
    "\n",
    "Now that we've become a bit familiar with the dataset, we can move on to building the spam filter.\n",
    "\n",
    "However, before creating it, it's very helpful to first think of a way of testing how well it works. When creating software (a spam filter is software), a good rule of thumb is that designing the test comes before creating the software. If we write the software first, then it's tempting to come up with a biased test just to make sure the software passes it.\n",
    "\n",
    "Once our spam filter is done, we'll need to test how good it is with classifying new messages. To test the spam filter, we're first going to split our dataset into two categories:\n",
    "\n",
    "- A training set, which we'll use to \"train\" the computer how to classify messages.\n",
    "- A test set, which we'll use to test how good the spam filter is with classifying new messages.\n",
    "\n",
    "We're going to keep 80% of our dataset for training, and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c72b42a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_random = sms.sample(frac=1, random_state=1)\n",
    "size = sms_random.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da9fd1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     86.53803\n",
       "spam    13.46197\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = sms_random.iloc[:int(size/100*80)].copy()\n",
    "training.reset_index()\n",
    "training['Label'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8d905e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     86.816143\n",
       "spam    13.183857\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = sms_random.iloc[int(size/100*80):].copy()\n",
    "test.reset_index()\n",
    "test['Label'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e21554",
   "metadata": {},
   "source": [
    "We can see that the two datasets we extracted have approximately the same proportion of spam and non-spam messages as the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb1c343",
   "metadata": {},
   "source": [
    "## Letter Case and Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc81e311",
   "metadata": {},
   "source": [
    "To calculate all the probabilities, we'll first need to perform a bit of data cleaning to bring the data in a format that will allow us to extract easily all the information we need. \n",
    "\n",
    "<img src=\"transformation.png\"/>\n",
    "\n",
    "We'll start with: \n",
    "- All words in the vocabulary are in lower case, so `SECRET` and `secret` come to be considered to be the same word.\n",
    "- Punctuation is not taken into account anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "686ce4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1078                          yep by the pretty sculpture\n",
       "4028          yes princess are you going to make me moan \n",
       "958                            welp apparently he retired\n",
       "4642                                              havent \n",
       "4674    i forgot 2 ask ü all smth  there s a card on d...\n",
       "                              ...                        \n",
       "4255                 how about clothes jewelry and trips \n",
       "1982    sorry i ll call later in meeting any thing rel...\n",
       "5180    babe i fucking love you too  you know fuck it ...\n",
       "4020    u ve been selected to stay in 1 of 250 top bri...\n",
       "371     hello my boytoy   geeee i miss you already and...\n",
       "Name: SMS, Length: 4457, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training['SMS'] = training['SMS'].str.replace('\\W', ' ', regex=True)\n",
    "training['SMS'] = training['SMS'].str.replace('\\s{2}', ' ', regex=True)\n",
    "training['SMS'] = training['SMS'].str.lower().copy()\n",
    "training['SMS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e20896",
   "metadata": {},
   "source": [
    "## Creating the Vocabulary\n",
    "\n",
    "We'll create a list with all of the unique words (**vocabulary**) that occur in the messages of our training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14997798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['steed',\n",
       " 'vibrant',\n",
       " 'took',\n",
       " 'hme',\n",
       " 'everybody',\n",
       " 'tablets',\n",
       " 'invite',\n",
       " 'token',\n",
       " 'lk',\n",
       " 'ahmad']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = []\n",
    "for sms in training['SMS'].str.split():\n",
    "    for word in sms:\n",
    "        vocabulary.append(word)\n",
    "vocabulary = list(set(vocabulary))\n",
    "vocabulary[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69c77e8",
   "metadata": {},
   "source": [
    "## The Final Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9454cb74",
   "metadata": {},
   "source": [
    "Now we're going to use the vocabulary to make the data transformation we need:\n",
    "\n",
    "<img src=\"transformation.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e2b3d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_per_sms = {unique_word: [0] * len(training['SMS']) for unique_word in vocabulary}\n",
    "for i, sms in enumerate(training['SMS']):\n",
    "    for word in sms:\n",
    "        if word in word_counts_per_sms:\n",
    "            word_counts_per_sms[word][i] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9454ece6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steed</th>\n",
       "      <th>vibrant</th>\n",
       "      <th>took</th>\n",
       "      <th>hme</th>\n",
       "      <th>everybody</th>\n",
       "      <th>tablets</th>\n",
       "      <th>invite</th>\n",
       "      <th>token</th>\n",
       "      <th>lk</th>\n",
       "      <th>ahmad</th>\n",
       "      <th>...</th>\n",
       "      <th>pub</th>\n",
       "      <th>skateboarding</th>\n",
       "      <th>goes</th>\n",
       "      <th>audiitions</th>\n",
       "      <th>corrct</th>\n",
       "      <th>entire</th>\n",
       "      <th>endowed</th>\n",
       "      <th>unmits</th>\n",
       "      <th>advice</th>\n",
       "      <th>window</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4452</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4453</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4454</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4455</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4456</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4457 rows × 7782 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      steed  vibrant  took  hme  everybody  tablets  invite  token  lk  ahmad  \\\n",
       "0         0        0     0    0          0        0       0      0   0      0   \n",
       "1         0        0     0    0          0        0       0      0   0      0   \n",
       "2         0        0     0    0          0        0       0      0   0      0   \n",
       "3         0        0     0    0          0        0       0      0   0      0   \n",
       "4         0        0     0    0          0        0       0      0   0      0   \n",
       "...     ...      ...   ...  ...        ...      ...     ...    ...  ..    ...   \n",
       "4452      0        0     0    0          0        0       0      0   0      0   \n",
       "4453      0        0     0    0          0        0       0      0   0      0   \n",
       "4454      0        0     0    0          0        0       0      0   0      0   \n",
       "4455      0        0     0    0          0        0       0      0   0      0   \n",
       "4456      0        0     0    0          0        0       0      0   0      0   \n",
       "\n",
       "      ...  pub  skateboarding  goes  audiitions  corrct  entire  endowed  \\\n",
       "0     ...    0              0     0           0       0       0        0   \n",
       "1     ...    0              0     0           0       0       0        0   \n",
       "2     ...    0              0     0           0       0       0        0   \n",
       "3     ...    0              0     0           0       0       0        0   \n",
       "4     ...    0              0     0           0       0       0        0   \n",
       "...   ...  ...            ...   ...         ...     ...     ...      ...   \n",
       "4452  ...    0              0     0           0       0       0        0   \n",
       "4453  ...    0              0     0           0       0       0        0   \n",
       "4454  ...    0              0     0           0       0       0        0   \n",
       "4455  ...    0              0     0           0       0       0        0   \n",
       "4456  ...    0              0     0           0       0       0        0   \n",
       "\n",
       "      unmits  advice  window  \n",
       "0          0       0       0  \n",
       "1          0       0       0  \n",
       "2          0       0       0  \n",
       "3          0       0       0  \n",
       "4          0       0       0  \n",
       "...      ...     ...     ...  \n",
       "4452       0       0       0  \n",
       "4453       0       0       0  \n",
       "4454       0       0       0  \n",
       "4455       0       0       0  \n",
       "4456       0       0       0  \n",
       "\n",
       "[4457 rows x 7782 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count = pd.DataFrame(word_counts_per_sms)\n",
    "words_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f670f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>steed</th>\n",
       "      <th>vibrant</th>\n",
       "      <th>took</th>\n",
       "      <th>hme</th>\n",
       "      <th>everybody</th>\n",
       "      <th>tablets</th>\n",
       "      <th>invite</th>\n",
       "      <th>token</th>\n",
       "      <th>...</th>\n",
       "      <th>pub</th>\n",
       "      <th>skateboarding</th>\n",
       "      <th>goes</th>\n",
       "      <th>audiitions</th>\n",
       "      <th>corrct</th>\n",
       "      <th>entire</th>\n",
       "      <th>endowed</th>\n",
       "      <th>unmits</th>\n",
       "      <th>advice</th>\n",
       "      <th>window</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go until jurong point crazy  available only in...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar  joking wif u oni</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say so early hor  u c already then say</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah i don t think he goes to usf he lives arou...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>this is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>will ü b going to esplanade fr home</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>pity  was in mood for that so  any other sugge...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>the guy did some bitching but i acted like i d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>rofl its true to its name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5341 rows × 7784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                                SMS  steed  vibrant  \\\n",
       "0      ham  go until jurong point crazy  available only in...    0.0      0.0   \n",
       "1      ham                         ok lar  joking wif u oni      0.0      0.0   \n",
       "2      NaN                                                NaN    0.0      0.0   \n",
       "3      ham     u dun say so early hor  u c already then say      0.0      0.0   \n",
       "4      ham  nah i don t think he goes to usf he lives arou...    0.0      0.0   \n",
       "...    ...                                                ...    ...      ...   \n",
       "5567  spam  this is the 2nd time we have tried 2 contact u...    NaN      NaN   \n",
       "5568   ham               will ü b going to esplanade fr home     NaN      NaN   \n",
       "5569   ham  pity  was in mood for that so  any other sugge...    NaN      NaN   \n",
       "5570   ham  the guy did some bitching but i acted like i d...    NaN      NaN   \n",
       "5571   ham                          rofl its true to its name    NaN      NaN   \n",
       "\n",
       "      took  hme  everybody  tablets  invite  token  ...  pub  skateboarding  \\\n",
       "0      0.0  0.0        0.0      0.0     0.0    0.0  ...  0.0            0.0   \n",
       "1      0.0  0.0        0.0      0.0     0.0    0.0  ...  0.0            0.0   \n",
       "2      0.0  0.0        0.0      0.0     0.0    0.0  ...  0.0            0.0   \n",
       "3      0.0  0.0        0.0      0.0     0.0    0.0  ...  0.0            0.0   \n",
       "4      0.0  0.0        0.0      0.0     0.0    0.0  ...  0.0            0.0   \n",
       "...    ...  ...        ...      ...     ...    ...  ...  ...            ...   \n",
       "5567   NaN  NaN        NaN      NaN     NaN    NaN  ...  NaN            NaN   \n",
       "5568   NaN  NaN        NaN      NaN     NaN    NaN  ...  NaN            NaN   \n",
       "5569   NaN  NaN        NaN      NaN     NaN    NaN  ...  NaN            NaN   \n",
       "5570   NaN  NaN        NaN      NaN     NaN    NaN  ...  NaN            NaN   \n",
       "5571   NaN  NaN        NaN      NaN     NaN    NaN  ...  NaN            NaN   \n",
       "\n",
       "      goes  audiitions  corrct  entire  endowed  unmits  advice  window  \n",
       "0      0.0         0.0     0.0     0.0      0.0     0.0     0.0     0.0  \n",
       "1      0.0         0.0     0.0     0.0      0.0     0.0     0.0     0.0  \n",
       "2      0.0         0.0     0.0     0.0      0.0     0.0     0.0     0.0  \n",
       "3      0.0         0.0     0.0     0.0      0.0     0.0     0.0     0.0  \n",
       "4      0.0         0.0     0.0     0.0      0.0     0.0     0.0     0.0  \n",
       "...    ...         ...     ...     ...      ...     ...     ...     ...  \n",
       "5567   NaN         NaN     NaN     NaN      NaN     NaN     NaN     NaN  \n",
       "5568   NaN         NaN     NaN     NaN      NaN     NaN     NaN     NaN  \n",
       "5569   NaN         NaN     NaN     NaN      NaN     NaN     NaN     NaN  \n",
       "5570   NaN         NaN     NaN     NaN      NaN     NaN     NaN     NaN  \n",
       "5571   NaN         NaN     NaN     NaN      NaN     NaN     NaN     NaN  \n",
       "\n",
       "[5341 rows x 7784 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_final = pd.concat([training,words_count], axis=1)\n",
    "training_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3015124d",
   "metadata": {},
   "source": [
    "## Calculating Constants First\n",
    "\n",
    "Now that we're done with data cleaning and have a training set to work with, we can begin creating the spam filter. Recall that the Naive Bayes algorithm will need to know the probability values of the two equations below to be able to classify new messages:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aa600e",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36190e76",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(Ham | w_1,w_2, ..., w_n) \\propto P(Ham) \\cdot \\prod_{i=1}^{n}P(w_i|Ham)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfdd314",
   "metadata": {},
   "source": [
    "Also, to calculate P(wi|Spam) and P(wi|Ham) inside the formulas above, recall that we need to use these equations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6e17e1",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e05c0d",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c618af2b",
   "metadata": {},
   "source": [
    "Some of the terms in the four equations above will have the same value for every new message. As a start, let's first calculate:\n",
    "\n",
    "- P(Spam) and P(Ham)\n",
    "- NSpam, NHam, NVocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf575803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11233851338700618"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_spam = len(training_final[training_final['Label'] == 'spam']) / len(training_final) \n",
    "p_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "14d4efe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8876614866129938"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_ham = 1 - p_spam\n",
    "p_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a35e3a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15190"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_words = training_final[training_final['Label'] == 'spam']['SMS'].str.split().apply(len)\n",
    "n_spam = spam_words.sum()\n",
    "n_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3171b030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57233"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_words = training_final[training_final['Label'] == 'ham']['SMS'].str.split().apply(len)\n",
    "n_ham = ham_words.sum()\n",
    "n_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c3b7f11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7782"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_vocabulary = len(vocabulary)\n",
    "n_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a41396d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16a7cc8",
   "metadata": {},
   "source": [
    "## Calculating Parameters\n",
    "\n",
    "For P(wi|Spam) and P(wi|Ham) will vary depending on the individual words. For instance, P(\"secret\"|Spam) will have a certain probability value, while P(\"cousin\"|Spam) or P(\"lovely\"|Spam) will most likely have other values.\n",
    "\n",
    "Although both P(wi|Spam) and P(wi|Ham) vary depending on the word, the probability for each individual word is constant for every new message.\n",
    "\n",
    "For instance, let's say we receive two new messages:\n",
    "\n",
    "- \"secret code\"\n",
    "- \"secret party 2night\"\n",
    "\n",
    "We'll need to calculate P(\"secret\"|Spam) for both these messages, and we can use the training set to get the values.\n",
    "\n",
    "In more technical language, the probability values that P(wi|Spam) and P(wi|Ham) will take are called parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8f63bf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_parameters = {word: 0 for word in vocabulary}\n",
    "ham_parameters = {word: 0 for word in vocabulary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c59276ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_messages = training_final[training_final['Label'] == 'spam']\n",
    "ham_messasges = training_final[training_final['Label'] == 'ham']\n",
    "\n",
    "for word in vocabulary:\n",
    "    n_word_spam = spam_messages[word].sum()\n",
    "    p_word_given_spam = (n_word_spam + alpha) / (n_spam + (alpha * n_vocabulary))\n",
    "    spam_parameters[word]  = p_word_given_spam\n",
    "    \n",
    "    n_word_ham = ham_messasges[word].sum()\n",
    "    p_word_given_ham = (n_word_ham + alpha) / (n_ham + (alpha * n_vocabulary)) \n",
    "    ham_parameters[word] = p_word_given_ham"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
